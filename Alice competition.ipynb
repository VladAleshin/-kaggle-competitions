{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import hstack\n",
    "import eli5\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "from scipy.stats.mstats import gmean\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.chdir('./')\n",
    "SEED = 17\n",
    "time_split = TimeSeriesSplit(n_splits=10)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=False, random_state=SEED)\n",
    "logit = LogisticRegression(C=1, random_state=SEED, solver='liblinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train, sparse_test, y, train_times, test_times, vectorizer, train_n_sites, test_n_sites \\\n",
    "                                            = prepare_sparse_features()\n",
    "scaler = MinMaxScaler()\n",
    "sparse_train, sparse_test, feature_names = add_time_features(train_times, test_times,\n",
    "                                            sparse_train, sparse_test, scaler, train_n_sites, test_n_sites)\n",
    "\n",
    "cv_scores = train_and_predict(logit, sparse_train, y, sparse_test, \n",
    "                             site_feature_names=vectorizer.get_feature_names(),\n",
    "                             new_feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare time features\n",
    "def add_time_features(train_times, test_times, X_sparse_train, X_sparse_test,\n",
    "                                                  scaler, n_sites_train, n_sites_test): \n",
    "          \n",
    "    times = ['time%s' % i for i in range(1, 11)]\n",
    "    train_times_len = len(train_times)\n",
    "    full_times = pd.concat([train_times, test_times])    \n",
    "    time_df = pd.DataFrame(index=full_times.index)   \n",
    "    \n",
    "    month = full_times['time1'].apply(lambda x: x.month)\n",
    "    day = full_times['time1'].apply(lambda x: x.day)   \n",
    "    hour = full_times['time1'].apply(lambda x: x.hour)\n",
    "    dayofweek = full_times['time1'].apply(lambda x: x.dayofweek)\n",
    "    min_val = full_times[times].min(axis=1)\n",
    "    max_val = full_times[times].max(axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    time_df['month_day_11_12'] = ((month == 11) & (day == 12)).astype('int')    \n",
    "    \n",
    "    \n",
    "    time_df['year'] = full_times['time1'].apply(lambda x: x.year)\n",
    "    time_df['duration'] = ((max_val - min_val) / np.timedelta64(1, 's')) / 1800\n",
    "    \n",
    "    time_df['day_3'] = (day == 3).astype('int')\n",
    "    time_df['day_21'] = (day == 21).astype('int')\n",
    "    time_df['day_26'] = (day == 26).astype('int')    \n",
    "    time_df['day_23'] = (day == 23).astype('int')\n",
    "    \n",
    "    \n",
    "    time_df['month_10'] = (month == 10).astype('int')     \n",
    "    \n",
    "    time_df['isWednesday'] = (dayofweek == 2).astype('int')\n",
    "    time_df['isSunday'] = (dayofweek == 6).astype('int')   \n",
    "    time_df['19-08h'] = (((hour >= 19) & (hour <= 23)) | ((hour >= 0) & (hour <= 8))).astype('int')\n",
    "    time_df['10-11h'] = ((hour >= 10) & (hour <= 11)).astype('int')\n",
    "    time_df['14h'] = ((hour >= 14) & (hour <= 14)).astype('int')\n",
    "    time_df['month12_day_greater_18'] = ((month == 12) & (day >= 18)).astype('int')\n",
    "    \n",
    "    \n",
    "    time_df['month4_day_greater_15'] = ((month == 4) & (day > 15)).astype('int')\n",
    "    col_for_dummies = ['year']  \n",
    "    time_df_dummies = pd.get_dummies(time_df, columns=col_for_dummies)\n",
    "    \n",
    "    \n",
    "    X_sparse_train_full = hstack([X_sparse_train, time_df_dummies[:train_times_len]])\n",
    "    X_sparse_test_full = hstack([X_sparse_test, time_df_dummies[train_times_len:]])\n",
    "        \n",
    "    return X_sparse_train_full, X_sparse_test_full, list(time_df_dummies.columns) \n",
    "\n",
    "\n",
    "#train model\n",
    "def train_and_predict(model, X_train, y_train, X_test, site_feature_names=vectorizer.get_feature_names(), \n",
    "                      new_feature_names=None, cv=time_split, scoring='roc_auc',\n",
    "                      top_n_features_to_show=30):\n",
    "   \n",
    "    cv_scores1 = cross_val_score(model, X_train, y_train, cv=cv, \n",
    "                            scoring=scoring, n_jobs=-1)\n",
    "    cv_scores2 = cross_val_score(model, X_train, y_train, cv=skf, \n",
    "                            scoring=scoring, n_jobs=-1)    \n",
    "    \n",
    "    print('CV scores time split', cv_scores1)\n",
    "    print('CV scores skf', cv_scores2)\n",
    "    print('')\n",
    "    print('CV mean 1: {}, CV std 1: {}'.format(round(cv_scores1.mean(), 5), round(cv_scores1.std(), 5)))\n",
    "    print('CV mean 2: {}, CV std 2: {}'.format(round(cv_scores2.mean(), 5), round(cv_scores2.std(), 5)))\n",
    "    \n",
    "    print('gmean: {}'.format(gmean([cv_scores1.mean(), cv_scores2.mean()])))    \n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    if new_feature_names:\n",
    "        all_feature_names = site_feature_names + new_feature_names \n",
    "    else: \n",
    "        all_feature_names = site_feature_names\n",
    "    \n",
    "    display_html(eli5.show_weights(estimator=model, \n",
    "                  feature_names=all_feature_names, top=top_n_features_to_show))\n",
    "    \n",
    "    if new_feature_names:\n",
    "        print('New feature weights:')\n",
    "        df1 = pd.DataFrame({'feature': new_feature_names, \n",
    "                        'coef': model.coef_.flatten()[-len(new_feature_names):]}).sort_values(by='coef', \n",
    "              ascending=False) \n",
    "    \n",
    "        print(df1)\n",
    "    \n",
    "    test_pred = model.predict_proba(X_test)[:, 1]\n",
    "    write_to_submission_file(test_pred) \n",
    "    \n",
    "    return cv_scores1\n",
    "\n",
    "def prepare_sparse_features():\n",
    "    print('Start...')    \n",
    "    scaler = MinMaxScaler()\n",
    "    times = ['time%s' % i for i in range(1, 11)]\n",
    "    sites = ['site%s' % i for i in range(1, 11)]\n",
    "    \n",
    "    train_df = pd.read_csv('train_sessions.csv',\n",
    "                           index_col='session_id', parse_dates=times)\n",
    "    test_df = pd.read_csv('test_sessions.csv',\n",
    "                          index_col='session_id', parse_dates=times)           \n",
    "    \n",
    "    train_df = train_df.sort_values(by='time1')\n",
    "    y = train_df.target.values\n",
    "    with open('site_dic.pkl', 'rb') as f:\n",
    "        site2id = pickle.load(f)\n",
    "\n",
    "    id2site = { v:k for (k, v) in site2id.items() }\n",
    "    id2site[0] = 'unknown'\n",
    "    for key, value in id2site.items():\n",
    "        id2site[key] = re.sub(\"^\\S*?\\.*?www\\S*?\\.\", '', value) \n",
    "        \n",
    "    sites = ['site%s' % i for i in range(1, 11)]    \n",
    "\n",
    "    #train\n",
    "    train_sessions = train_df[sites].fillna(0).astype(np.int32).apply(lambda row:\n",
    "                                                    ' '.join([id2site[i] for i in row]), axis=1)    \n",
    "    train_n_unique_sites = train_sessions.apply(lambda x: \n",
    "                                len(np.unique(x.replace('unknown', '').split()))).values.reshape(-1, 1) \n",
    "    \n",
    "    train_n_sites = train_sessions.apply(lambda x: \n",
    "                                len(x.replace('unknown', '').split())).values.reshape(-1, 1)  \n",
    "    \n",
    "    train_n_sites_scaled = train_n_sites / 10      \n",
    "\n",
    "    #test    \n",
    "    test_sessions = test_df[sites].fillna(0).astype(np.int32).apply(lambda row: \n",
    "                                                    ' '.join([id2site[i] for i in row]), axis=1)    \n",
    "    \n",
    "    test_n_sites = test_sessions.apply(lambda x: \n",
    "                            len(x.replace('unknown', '').split())).values.reshape(-1, 1)    \n",
    "    \n",
    "    test_n_sites_scaled = test_n_sites / 10    \n",
    "   \n",
    "    vectorizer = TfidfVectorizer(max_features=50000, \n",
    "                                 ngram_range=(1, 5), tokenizer=lambda x: x.split())\n",
    "\n",
    "    sparse_train = vectorizer.fit_transform(train_sessions)\n",
    "    sparse_test = vectorizer.transform(test_sessions)    \n",
    "    \n",
    "    full_train = hstack([sparse_train]).tocsr()\n",
    "    full_test = hstack([sparse_test]).tocsr()\n",
    "    \n",
    "    train_times, test_times = train_df[times], test_df[times]    \n",
    "    \n",
    "    return full_train, full_test, y, train_times, test_times, vectorizer, train_n_sites, test_n_sites     \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
